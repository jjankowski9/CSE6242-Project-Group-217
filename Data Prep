import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE

# Load dataset
df = pd.read_csv('creditcard.csv')

# Data Load Check
print(f"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")

# 1. Initial Exploration
print("\n--- Data Overview ---")
print(df.head())
print("\n--- Missing Values ---")
print(df.isnull().sum().sum())
print("\n--- Class Distribution ---")
print(df['Class'].value_counts())
print(f"Fraud rate: {df['Class'].mean()*100:.2f}%")

## We have a very small (0.17%) fraud rate, indicating a highly imbalanced dataset.
## We will need to handle this imbalance using SMOTE

# 2. Handle Missing Values
df = df.dropna()  # Remove any rows with missing values
print(f"\n✓ After handling missing values: {len(df):,} rows")

# 3. Remove Duplicates
initial_rows = len(df)
df = df.drop_duplicates()
print(f"✓ Removed {initial_rows - len(df)} duplicates")

# 4. Feature Engineering

# Time-based features
# Time = seconds elapsed since first transaction
# Convert to hours elapsed
df['Time_Hours'] = df['Time'] / 3600
    
# Convert to days elapsed
df['Time_Days'] = df['Time'] / (3600 * 24)
    
# Create time period bins (early/middle/late in dataset)
# This helps capture temporal patterns in fraud over the collection period
df['Time_Period'] = pd.cut(df['Time'], 
                            bins=4, 
                            labels=['Period_1', 'Period_2', 'Period_3', 'Period_4'])
df['Time_Period_Encoded'] = df['Time_Period'].cat.codes
    
print(f"✓ Time represents {df['Time_Hours'].max():.1f} hours ({df['Time_Days'].max():.1f} days) of transactions")
print(f"✓ Created time-based features: Time_Hours, Time_Days, Time_Period")


# X. Handling Class Imbalance with SMOTE
# Log transformation (handles zeros with log1p)
df['Amount_Log'] = np.log1p(df['Amount'])
    
# Amount categories
df['Amount_Category'] = pd.cut(df['Amount'], 
                                bins=[0, 10, 50, 100, 500, float('inf')],
                                labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])
df['Amount_Category_Encoded'] = df['Amount_Category'].cat.codes
    
print("✓ Created amount-based features: Amount_Log, Amount_Category")

# Time since last transaction (within the dataset sequence)
df['Time_Delta'] = df['Time'].diff().fillna(0)
    
# Rolling statistics (transactions per time window)
# Note: This assumes data is sorted by Time
df = df.sort_values('Time').reset_index(drop=True)
    
print("✓ Created temporal sequence features")

# 5. Feature Scaling
# Scale Amount, Time, and derived time features
scaler = RobustScaler()

df['Amount_Scaled'] = scaler.fit_transform(df[['Amount']])
df['Time_Scaled'] = scaler.fit_transform(df[['Time']])
df['Time_Hours_Scaled'] = scaler.fit_transform(df[['Time_Hours']])
df['Time_Delta_Scaled'] = scaler.fit_transform(df[['Time_Delta']])

# 6. Test-Train Split
# Exclude: original Time, Amount, Class, and categorical string columns
exclude_cols = ['Class', 'Amount', 'Time', 'Time_Period', 'Amount_Category']
feature_cols = [col for col in df.columns if col not in exclude_cols]

X = df[feature_cols]
y = df['Class']

print(f"\nFeatures used: {len(feature_cols)}")
print(f"Sample features: {feature_cols[:10]}")

# Option 1: Random split (maintains distribution)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

print(f"\nTrain set: {len(X_train):,} samples")
print(f"Test set: {len(X_test):,} samples")
print(f"Train fraud rate: {y_train.mean()*100:.4f}%")
print(f"Test fraud rate: {y_test.mean()*100:.4f}%")

# Option 2: Temporal split (more realistic - uncomment to use)
# This tests if your model can predict future fraud based on past data
split_point = int(len(df) * 0.7)
X_train2 = X.iloc[:split_point]
X_test2 = X.iloc[split_point:]
y_train2 = y.iloc[:split_point]
y_test2 = y.iloc[split_point:]

print("Using TEMPORAL split (first 70% train, last 30% test)")
print(f"\nTrain set: {len(X_train2):,} samples")
print(f"Test set: {len(X_test2):,} samples")
print(f"Train fraud rate: {y_train2.mean()*100:.4f}%")
print(f"Test fraud rate: {y_test2.mean()*100:.4f}%")

# 7. Handling Class Imbalance with SMOTE
print("\nOriginal training set distribution:")
print(y_train.value_counts())

smote = SMOTE(sampling_strategy=0.5, random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"\nBalanced training set distribution:")
print(pd.Series(y_train_balanced).value_counts())

print(f"\n✓ SMOTE applied successfully")
print(f"  Original train size: {len(X_train):,}")
print(f"  Balanced train size: {len(X_train_balanced):,}")
print(f"  Increase: {len(X_train_balanced) - len(X_train):,} samples")


# 8. Saving Preprocessed Data
# Save test set (keep original distribution)
test_df = pd.concat([X_test.reset_index(drop=True), 
                     y_test.reset_index(drop=True)], axis=1)
test_df.to_csv('test_data_processed.csv', index=False)

# Save balanced training set
train_balanced_df = pd.concat([
    pd.DataFrame(X_train_balanced, columns=X_train.columns),
    pd.Series(y_train_balanced, name='Class')
], axis=1)
train_balanced_df.to_csv('train_data_balanced.csv', index=False)

# Save unbalanced training set as well (for comparison)
train_df = pd.concat([X_train.reset_index(drop=True), 
                      y_train.reset_index(drop=True)], axis=1)
train_df.to_csv('train_data_original.csv', index=False)

#9. Summary Stats
print("\n✓ Saved processed data files:")
print("  - train_data_original.csv (imbalanced)")
print("  - train_data_balanced.csv (SMOTE applied)")
print("  - test_data_processed.csv")

print("\n" + "="*70)
print("PREPROCESSING SUMMARY")
print("="*70)
print(f"Original dataset: {initial_rows:,} rows")
print(f"After cleaning: {len(df):,} rows")
print(f"Features created: {len(feature_cols)}")
print(f"Training samples (original): {len(X_train):,}")
print(f"Training samples (balanced): {len(X_train_balanced):,}")
print(f"Test samples: {len(X_test):,}")
print(f"\nOriginal fraud rate: {df['Class'].mean()*100:.4f}%")
print(f"Train fraud rate (balanced): {y_train_balanced.mean()*100:.2f}%")
print(f"Test fraud rate: {y_test.mean()*100:.4f}%")
print("="*70)

# 10. Feature Importance  
print("\n--- Quick Feature Analysis ---")
print("\nTop features by correlation with fraud:")
correlations = df[feature_cols + ['Class']].corr()['Class'].abs().sort_values(ascending=False)
print(correlations.head(10))

print("\n✓ Preprocessing complete! Ready for modeling.")
print("\nRECOMMENDED MODELS:")
print("  1. Logistic Regression (baseline)")
print("  2. Random Forest Classifier")
print("  3. XGBoost Classifier (recommended)")
print("  4. LightGBM (fast & accurate)")
print("\nREMEMBER: Use precision, recall, F1, and AUC-ROC for evaluation!")
